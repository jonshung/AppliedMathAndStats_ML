{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Linear Regression</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Anaconda native modules\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from tabulate import tabulate\n",
    "from statistics import NormalDist, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction and partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "# Source: MSc. Nguyen Ngoc, Toan\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Extracting data features and targeting dependent variable ground truth values for training and testing\n",
    "X_train = train.iloc[:, :-1]    # Dataframe (Containing 5 training features)\n",
    "y_train = train.iloc[:, -1]     # Series    (Containing 1 training target)\n",
    "\n",
    "X_test = test.iloc[:, :-1]      # Dataframe (Containing 5 testing features)\n",
    "y_test = test.iloc[:, -1]       # Series    (Containing 1 testing target\n",
    "\n",
    "# A helper list to stringify feature descriptions\n",
    "feature_descriptions = ['Hours Studied', 'Previous Scores', 'Extracurricular Activities Participation', 'Sleep Hours', 'Sample Question Papers Practiced,']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 4's template OLS Linear Regression model\n",
    "class OLSLinearRegression:\n",
    "    w = np.ndarray\n",
    "    ridge_lambda: float | None = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' \n",
    "        This function is used to fit the model to the data. It uses the Ordinary Least Squares method to find the optimal parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data\n",
    "        y : np.array\n",
    "            Output data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance of the class\n",
    "        \n",
    "        '''\n",
    "        X = np.c_[ X, np.ones(X.shape[0]) ] # appending a constant of unexplained parameter\n",
    "        if self.ridge_lambda is not None:\n",
    "            X_pinv = np.linalg.inv(X.T @ X + self.ridge_lambda*np.eye(X.shape[1])) @ X.T # see reference\n",
    "        else:\n",
    "            X_pinv = np.linalg.inv(X.T @ X) @ X.T    # np.linalg.pinv(X)\n",
    "        self.w = X_pinv @ y\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def get_params(self):\n",
    "        ''' \n",
    "        This function is used to get the parameters of the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.w : np.array\n",
    "            Optimal parameters (column vector)\n",
    "        '''\n",
    "\n",
    "        return self.w\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' \n",
    "        This function is used to predict the output of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X @ self.w : np.array\n",
    "            Predicted output\n",
    "        '''\n",
    "        X = np.c_[ X, np.ones(X.shape[0]) ] # appending a constant of unexplained parameter\n",
    "        return X @ self.w\n",
    "    \n",
    "    def latex_repr(self, centered=False) -> Latex:\n",
    "        model_repr = ('$$' if centered else '$') + '\\\\hat{{y}}='\n",
    "        params = self.get_params()\n",
    "        for i in range(len(params)):\n",
    "            model_repr += '-' if params[i] < 0 else ('+' if i > 0 else '')\n",
    "            if i > 0:\n",
    "                model_repr += ' '\n",
    "            model_repr += '{param:.3f}'.format(param=np.abs(params[i]))\n",
    "            if(i < len(params) - 1):\n",
    "                model_repr += 'x_{id} '.format(id=i+1)\n",
    "        model_repr += '$$' if centered else '$'\n",
    "        return Latex(model_repr)\n",
    "    \n",
    "\n",
    "def ols_fitness_stats(M: OLSLinearRegression, X, Y, print_statistic=False) -> tuple[float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Validating the model using a testing set and returns \n",
    "            Mean (Summed) Absolute error, \n",
    "            Max Error, \n",
    "            Min Error, \n",
    "            Mean of Error, \n",
    "            Error Variance\n",
    "    statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The ordered testing data points. N is the number of data points, n is the number of features.\n",
    "    Y : Array-like, np.ndarray, shape (N)\n",
    "        The ground truth data of each ordered testing data points.\n",
    "    print_statistic : boolean\n",
    "        Print the returning statistic.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    MAE : float\n",
    "        The model's Mean (Summed) Absolute Error.\n",
    "    MxE : float\n",
    "        The model's MaXimum absolute Error.\n",
    "    MnE : float\n",
    "        The model's MiNimum absolute Error.\n",
    "    EM : float\n",
    "        The model's Error Mean.\n",
    "    Var : float\n",
    "        The model's error Variance.\n",
    "    \"\"\"\n",
    "    Err = Y.ravel()-M.predict(X).ravel()\n",
    "    AE = np.abs(Err)\n",
    "    EM = np.mean(Err)\n",
    "    MxE = np.max(AE)\n",
    "    MnE = np.min(AE)\n",
    "    test_len = Y.shape[0]\n",
    "    Var = variance(Err)\n",
    "\n",
    "    MAE = np.sum(AE)/test_len\n",
    "    if print_statistic:\n",
    "        display(Latex('$\\\\text{{MAE}}: {mae:.3f}$'.format(mae=MAE)))\n",
    "        display(Latex('$\\\\text{{max absolute error: }}{maxerr:.3f}$'.format(maxerr=MxE)))\n",
    "        display(Latex('$\\\\text{{min absolute error: }}{minerr:.3f}$'.format(minerr=MnE)))\n",
    "        display(Latex('$\\\\text{{Error mean: }}{varr:.3f}$'.format(varr=EM)))\n",
    "        display(Latex('$\\\\text{{Error variance: }}{varr:.3f}$'.format(varr=Var)))\n",
    "    return MAE, MxE, MnE, EM, Var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration on training set\n",
    "\n",
    "# preprocessing\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "if(X_train.ndim < 2):\n",
    "    X_train = X_train.reshape(X_train.shape + (1,))\n",
    "if(X_test.ndim < 2):\n",
    "    X_test = X_test.reshape(X_test.shape + (1,))\n",
    "\n",
    "def linear_regression_correlation(a, b):\n",
    "    \"\"\"\n",
    "    Single variable regression analysis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a, b : Array-like, np.ndarray shape (N)\n",
    "        The input independent and dependent variable vectors respectively.\n",
    "        The assumed regression model is the simple linear regression model: y = b1 + b0.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    b1 : float\n",
    "        The coefficient associated with the independent variable.\n",
    "    b0 : float\n",
    "        The constant terms of unexplained value of the model\n",
    "    r : float range [-1, 1]\n",
    "        The correlation coefficient, representing the explainability of b when depending on a.\n",
    "    \"\"\"\n",
    "    mean_x = a.sum()/a.shape[0]\n",
    "    mean_y = b.sum()/b.shape[0]\n",
    "    Sxy = np.sum((a-mean_x)@(b-mean_y))\n",
    "    Sxx = np.sum(np.square(a-mean_x))\n",
    "    Syy = np.sum(np.square(b-mean_y))\n",
    "    return (Sxy/Sxx, mean_y-(Sxy/Sxx)*mean_x, Sxy/np.sqrt(Sxx*Syy))\n",
    "\n",
    "def multivariable_approximate_curvature(X, Y, singular_color=None, singular_index=None):\n",
    "    \"\"\"\n",
    "    Plotting the independence variable(s) array x in Re^(d=n) and the resulting y in Re,\n",
    "    along with a linear regression line of the data pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        Array of input data points independence variable(s).\n",
    "    Y : Array-like, np.ndarray, shape (N)\n",
    "        Array of scalar response variable corresponds to the input data points.\n",
    "    singular_color : str\n",
    "        The single hex value representing the RGB color of the graph's marker.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        x_p = X[:, i]\n",
    "        linregr = linear_regression_correlation(x_p, Y)\n",
    "        y_tilde = linregr[1] + linregr[0]*x_p\n",
    "        if singular_color is None:\n",
    "            color = ax._get_lines.get_next_color()\n",
    "        else:\n",
    "            color = singular_color\n",
    "        ax.plot(x_p, Y, 'o', color=color, label= ('x{idx}'.format(idx=i+1) if singular_index is None else f'x{singular_index}') + ', r: {rv:.7f}'.format(rv=linregr[2]) )\n",
    "        ax.plot(x_p, y_tilde, '-', color=color)\n",
    "        ax.set_xlabel('x' if singular_index is None else f'x{singular_index}')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.set_title('Multivariable approximate curvature')\n",
    "    ax.legend()\n",
    "\n",
    "\"\"\" Plotting set\n",
    "for i in range(X_train.shape[1]+1):\n",
    "    if i == X_train.shape[1]:\n",
    "        multivariable_approximate_curvature(X_train[:, :], y_train[:])\n",
    "        continue\n",
    "    else:\n",
    "        color_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "        multivariable_approximate_curvature(X_train[:, i, np.newaxis], y_train[:], color_list[i], i+1)\n",
    "# \"\"\"\n",
    "\n",
    "def histogram_analysis(x):\n",
    "    \"\"\"\n",
    "    Plotting the histogram of a variable's distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Array-like, np.ndarray, shape (N)\n",
    "        Array of input data points univariable feature, i.e xi in Re.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(x)\n",
    "    ax.set_title('Variable distribution histogram')\n",
    "    ax.legend()\n",
    "    plt.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\"\"\" Plotting set\n",
    "for i in range(X_train.shape[1]]):\n",
    "    fig, ax = histogram_analysis(X_train[:, i])\n",
    "    fig.suptitle('x{i}'.format(i=i+1))\n",
    "# \"\"\"\n",
    "\n",
    "def bivariate_analysis(X, p):\n",
    "    \"\"\"\n",
    "    Plotting the the correlation between all none-ordered pairs of (p, i) for i <> p and i in [1, n], n is the number of features\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        Array of input data points independence variable(s).\n",
    "    p : integer, range [1, n]\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in range(X.shape[1]):\n",
    "        if((i+1) == p):\n",
    "            continue\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(X[:, p-1], X[:, i])\n",
    "        ax.set_xlabel(f'x{p}')\n",
    "        ax.set_ylabel(f'x{i+1}')\n",
    "        ax.set_title(f'Bivariate scatter plot of x{p} and x{i+1}')\n",
    "    plt.draw()\n",
    "\n",
    "\"\"\" Plotting set\n",
    "for i in range(X_train.shape[1]):\n",
    "    bivariate_analysis(X_train[:], i+1)\n",
    "# \"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate, simple, rank 1 polynomial linear regression of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_linregress(X, Y, print_model=True) -> OLSLinearRegression:\n",
    "    \"\"\"\n",
    "    Perform an Ordinary Least Squared Linear Regression using the independence variables as the basis linear function, having a maximum of 1 input variate nad\n",
    "    with a maximum polynomial level of 1 and f_i: R->R. Hence, this also is a linear function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The ordered training data points. N is the number of data points, n is the number of features.\n",
    "    Y : Array-like, np.ndarray, shape (N)\n",
    "        The ground truth data of each ordered data points.\n",
    "    print_model : boolean\n",
    "        Print the model mathematical expression.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    model : OLSLinearRegression\n",
    "        An OLSLinearRegression object wrapping the model's architecture and trained parameters.\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0] # making sure they inputs have the same data point sizes\n",
    "    # we mostly takes input as vectors of features, so to model it, we must also consider the constant term of the model.\n",
    "    # Hence, we stack another column after the last column of X\n",
    "    model = OLSLinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    if print_model:\n",
    "        display(model.latex_repr(True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the model's MAE\n",
    "mdl = ols_linregress(X_train, y_train)\n",
    "ols_fitness_stats(mdl, X_test, y_test, True)\n",
    "\n",
    "# \"\"\" Plot the resulting model\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "mark_truth = ax.plot(X_test, y_test, 'ro')\n",
    "mark_predict = ax.plot(X_test, mdl.predict(X_test), 'bo')\n",
    "ax.set_xlabel(f'x')\n",
    "ax.set_ylabel(f'y')\n",
    "ax.legend(handles=[mark_truth[0], mark_predict[0]], labels=['Ground truth data', 'Model prediction'])\n",
    "ax.set_title('Linear regression by OLS model based on all feature', loc=\"center\")\n",
    "plt.draw()\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Student Performance} = 2.852x_1 + 1.018x_2 + 0.604x_3 + 0.474x_4 + 0.192x_5 - 33.969\n",
    "$$\n",
    "Where $x_1$ is the hours studied, $x_2$ is the previous scores, $x_3$ is the extracurricular activities participation, $x_4$ is the sleep hours and $x_5$ is the number of sample question papers practied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single variable, simple, rank 1 polynomial linear regression for each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are trained under <i>k</i>-fold Cross validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing k-fold training for each model, then compared them using MAE and selecting the best model for further operations.\n",
    "\n",
    "def k_fold(X, Y, k, shuffle=False) -> tuple[OLSLinearRegression, float]:\n",
    "    \"\"\"\n",
    "    Perform K-fold cross validation process on covariate matrix X and response matrix Y, \n",
    "    returning a parameter-wise averaging model and the averaged MAE fitness value\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The covariate training matrix.\n",
    "    Y : Array-like, np.ndarray, shape (N)\n",
    "        The ground truth training response vector.\n",
    "    k : integer\n",
    "        The number of partition (fold) to be considered.\n",
    "    shuffle: boolean\n",
    "        Specify whether or not the procedure should perform a one-time permutation of the training set.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    avg_model : OLSLinearRegression\n",
    "        A parameter-wise averaged OLSLinearRegression model after training for k cross validation processes.\n",
    "    MAE : float\n",
    "        The averaged MAE fitness meter after k cross validation processes.\n",
    "    \"\"\"\n",
    "    k=int(k)\n",
    "    # not really used very often, just an utility\n",
    "    if shuffle:\n",
    "        assert X.shape[0] == Y.shape[0] # making sure they inputs have the same data point sizes\n",
    "        shuffle_indices = np.random.default_rng().permutation(X.shape[0])\n",
    "        X = X[shuffle_indices]\n",
    "        Y = Y[shuffle_indices]\n",
    "\n",
    "    \n",
    "    data_steps = int(X.shape[0] / k)\n",
    "    cross_validate_fitness = 0\n",
    "    model_params = np.zeros(X.shape[1]+1)\n",
    "    for k_step in range(k):\n",
    "        # overshadowing X_train defined in global space by the local k-th step training set\n",
    "        # No need to append a constant N size 1-vector here since ols_linregress already did that for us\n",
    "        X_train = np.r_[ X[0:data_steps*k_step], X[data_steps*(k_step+1):] ]\n",
    "        Y_train = np.r_[ Y[0:data_steps*k_step], Y[data_steps*(k_step+1):] ]\n",
    "        subsample_linear_model = ols_linregress(X_train, Y_train, False)\n",
    "\n",
    "        # Validation set\n",
    "        X_validate = X[data_steps*k_step:data_steps*(k_step+1)]\n",
    "        Y_validate = Y[data_steps*k_step:data_steps*(k_step+1)]\n",
    "\n",
    "        k_MAE = ols_fitness_stats(subsample_linear_model, X_validate, Y_validate)[0]\n",
    "        cross_validate_fitness += k_MAE\n",
    "        model_params += subsample_linear_model.get_params()\n",
    "\n",
    "    # Averaging after k-th steps of training and assign the averaged results to the model\n",
    "    linear_model = OLSLinearRegression()\n",
    "    model_params = model_params/k\n",
    "    linear_model.w = model_params\n",
    "    return linear_model, cross_validate_fitness / k\n",
    "\n",
    "def single_feature_k_fold_ols(X, Y, k) -> tuple[list[OLSLinearRegression], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform an Ordinary Least Squared Linear Regression on n model f_i: R->R, f_i(x_i) = a_i*x_i + b_i Where i is the i-th feature of the independence variable set;\n",
    "    and returns a list of OLSLinearRegression model and an array of MAE of each model. Both have length corresponding to n features.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The ordered training variables. N is the number of data points, n is the number of features.\n",
    "    Y : Array-like, np.ndarray, shape (N)\n",
    "        The ground truth data of each ordered data points.\n",
    "    k : integer\n",
    "        The number of training data fold (partition).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    models : list[OLSLinearRegression]\n",
    "        A list of OLSLinearRegression object wrapping the models' architectures and trained parameters.\n",
    "    MAE_list : np.ndarray((k,))\n",
    "        An 0-indexed array of averaged out MAE fitness corresponds to each i-th model in models.\n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0] # making sure they inputs have the same data point sizes\n",
    "    shuffle_indices = np.random.default_rng().permutation(X.shape[0])\n",
    "    X_shuffle = X[shuffle_indices]\n",
    "    Y_shuffle = Y[shuffle_indices]\n",
    "    models = []\n",
    "    fitnesses = np.ndarray(X.shape[1])\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        X_feature = X_shuffle[:, i, np.newaxis]\n",
    "        Y_feature = Y_shuffle # since our Y is scalar-typed vector\n",
    "        subsample_model, subsample_fitness = k_fold(X_feature, Y_feature, k)\n",
    "        # saving results\n",
    "        models.append(subsample_model)\n",
    "        # MAE Mean\n",
    "        fitnesses[i] = subsample_fitness\n",
    "    return models, fitnesses\n",
    "\n",
    "# cross validate results\n",
    "ms, fitnesses = single_feature_k_fold_ols(X_train, y_train, 10)\n",
    "# cross validation data representation\n",
    "models_repr = []\n",
    "for i, (m, fitness) in enumerate(zip(ms, fitnesses)):\n",
    "    models_repr.append('$\\hat{{y}}={b1:.3f}x_{i}'.format(i=i+1, b1=m.get_params()[0]) \\\n",
    "                  + ('+ {b0:.3f}$' if m.get_params()[1] > 0 else '{b0:.3f}$').format(b0=m.get_params()[1]))\n",
    "cross_validation_output = {\n",
    "    'Index': range(1,len(ms)+1),\n",
    "    'Feature Description': feature_descriptions,\n",
    "    'Model' : models_repr,\n",
    "    'MAE': np.round(fitnesses, 3)\n",
    "}\n",
    "cv_table = tabulate(cross_validation_output, headers='keys', tablefmt='github')\n",
    "display(Markdown('<center>\\n\\n' + cv_table + '\\n\\n</center>'))\n",
    "\n",
    "# Extracting the index of the best feature (since we have been following the structural run of the codebase, \n",
    "# one can assumes that the index of this list aligns with the index of the feature)\n",
    "best_index = np.argmin(fitnesses)\n",
    "display(Markdown('<center>\\n\\nBest candidate feature: ' + feature_descriptions[best_index] + '\\n\\n</center>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model\n",
    "\n",
    "X_best_train = X_train[:, best_index]\n",
    "# Best fit polynomial of degree 1 OLS Linear Regression model\n",
    "best_feature_model = ols_linregress(X_best_train, y_train, False)\n",
    "display(Latex('$$\\hat{{y}}={b1:.3f}x_{i}'.format(i=best_index+1, b1=best_feature_model.get_params()[0]) \\\n",
    "                  + ('+ {b0:.3f}$$' if best_feature_model.get_params()[1] > 0 else '{b0:.3f}$$').format(b0=best_feature_model.get_params()[1])))\n",
    "# \"\"\" Plot the resulting model\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(X_test[:, best_index], y_test, 'ro', label='Ground truth data')\n",
    "ax.plot(X_test[:, best_index], best_feature_model.predict(X_test[:, best_index]), 'b-', label='Model prediction')\n",
    "ax.set_xlabel(f'x{best_index+1}')\n",
    "ax.set_ylabel(f'y')\n",
    "ax.legend()\n",
    "ax.set_title('Linear regression by OLS model based on ' + feature_descriptions[best_index] + ' feature', loc=\"center\")\n",
    "plt.draw()\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Latex('$$\\\\text{{MAE:}} \\quad {mae:.3f}$$'.format(mae=ols_fitness_stats(best_feature_model, X_test[:, best_index], y_test)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Student Performance} = 1.011x_2+1.011$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis testing\n",
    "\n",
    "def hypothesis_test_feature_contribution(X, Y, X_test, Y_test, alpha) -> tuple[float, bool]:\n",
    "    \"\"\"\n",
    "    Perform hypothesis tesing by p-value of the simple linear regression model's coefficient b1 closing in on 0 \n",
    "    of a linear model with input of assumed independent univariable X, and scalar response vector Y\n",
    "\n",
    "    Note: a true evident doesn't really says much about the true facts of the feature, since outliers and missing data\n",
    "    could interfere with the process. But for the sakes of simplicity, it is an acceptable result that if\n",
    "    evident is True, then we can conclude the feature has significant contribution to the model, given a significance level alpha\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, 1)\n",
    "        The input training variable\n",
    "    Y : Array-like, np.ndarray, shape (n)\n",
    "        The input training response (ground truth) vector\n",
    "    X_test : Array-like, np.ndarray, shape (N, 1)\n",
    "        The input testing variable\n",
    "    Y_test : Array-like, np.ndarray, shape (n)\n",
    "        The input testing response (ground truth) vector\n",
    "    alpha : float\n",
    "        The significance level of the test\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    p : float\n",
    "        The statistic p-value, assuming a \n",
    "    evident : boolean\n",
    "        Whether the test yields enough evidence to prove the significance of the feature\n",
    "    \"\"\"\n",
    "    lin_regress_model_test = OLSLinearRegression()\n",
    "    lin_regress_model_test.fit(X, Y)\n",
    "    MSE = np.sum(np.square(Y_test.ravel()-lin_regress_model_test.predict(X_test).ravel()))/(X_test.shape[0])\n",
    "    \n",
    "    # Assuming the b1 converges to a normal distribution given our large dataset\n",
    "    X_test_mean = np.mean(X_test)\n",
    "    #sigma_b0 = np.sqrt(MSE)*np.sqrt((1/X_test.shape[0])+np.square(X_test_mean)/Sxx)  no need, since we only need b1 for linearlity test\n",
    "    sigma_b1 = np.sqrt(MSE/np.sum(np.square(X_test - X_test_mean)))\n",
    "    z0 = lin_regress_model_test.get_params()[0]/(sigma_b1)\n",
    "    p_value = 2*(1-NormalDist().cdf(np.abs(z0)))\n",
    "    return [p_value, p_value <= alpha]\n",
    "\n",
    "# performing hypothesis test on each feature\n",
    "def perform_hypothesis_test():\n",
    "    \"\"\" \n",
    "    Perform simple hypothesis testing on each feature's univariate simple linear regression model slope coefficient\n",
    "    alpha = 0.5\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    alpha = 0.05\n",
    "    for i in range(X_train.shape[1]):\n",
    "        p_value, evident = hypothesis_test_feature_contribution(X_train[:, i], y_train, X_test[:, i], y_test, alpha=alpha)\n",
    "        print(f\"alpha: {alpha}, p-value: {p_value}, evident: {evident}\")\n",
    "\n",
    "# y = ax1 + bx2\n",
    "def model_1_expression(X):\n",
    "    \"\"\"\n",
    "    C Model 1 matrix representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X' : Array-like, np.ndarray, shape (N, 2)\n",
    "        The resulting model's covariate matrix.\n",
    "    \"\"\"\n",
    "    return X[:, :2]\n",
    "\n",
    "def model_1(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct a C Model 1 OLS Linear Regression object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "    Y : Array-like, np.ndarray, Shape (N)\n",
    "        The response vector.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    model : OLSLinearRegression\n",
    "        C Model 1 object wrapper.\n",
    "    \"\"\"\n",
    "    X = model_1_expression(X)\n",
    "    model = OLSLinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "# ridge regression by L2 regularization form\n",
    "# multivariable_approximate_curvature(X_train[:, 0, np.newaxis]*X_train[:, 1, np.newaxis], y_train[:])\n",
    "# https://en.wikipedia.org/wiki/Anscombe_transform#Alternatives\n",
    "# https://en.wikipedia.org/wiki/Ridge_regression#Generalized_Tikhonov_regularization\n",
    "# y = 2asqrt(x1*x2+3/8) + b*x2 + c*x3 + d*(x4/x1) + e*x5 + err\n",
    "def model_2_expression(X):\n",
    "    \"\"\"\n",
    "    C Model 2 matrix representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X' : Array-like, np.ndarray, shape (N, 5)\n",
    "        The resulting model's covariate matrix.\n",
    "    \"\"\"\n",
    "    X1X2 = X[:, 0]*X[:, 1]\n",
    "    return np.c_[ 2*np.sqrt(X1X2 + 3/8), X[:, 1:3], X[:, 3]/X[:, 0], X[:, 4]]\n",
    "\n",
    "def model_2(X, Y, ridge_lambda, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct a C Model 2 OLS Linear Regression object while applying L2 Regularization with Tikhonov factor = ridge_lambda.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "    Y : Array-like, np.ndarray, Shape (N)\n",
    "        The response vector.\n",
    "    ridge_lambda : float\n",
    "        The Tikhonov factor used to bias the model coefficient in case of nonorthogonal covariate feature matrix\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    model : OLSLinearRegression\n",
    "        C Model 2 object wrapper.\n",
    "    \"\"\"\n",
    "    X = model_2_expression(X)\n",
    "    model = OLSLinearRegression()\n",
    "    model.ridge_lambda = ridge_lambda\n",
    "    model.fit(X, Y)\n",
    "    return model\n",
    "\n",
    "# y = a*x1 + b*x2 + c*( ( Norm(x1)/std_dev(Norm(x1)) ) * Norm(x2) ) + d*x3 + e*(1-x1/x4) + f*x5 + err\n",
    "def model_3_expression(X):\n",
    "    \"\"\"\n",
    "    C Model 3 matrix representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    X' : Array-like, np.ndarray, shape (N, 6)\n",
    "        The resulting model's covariate matrix.\n",
    "    \"\"\"\n",
    "    X1_norm = (X[:, 0] - np.mean(X[:, 0]))/np.sqrt(variance(X[:, 0]))\n",
    "    X2_norm = (X[:, 1] - np.mean(X[:, 1]))/np.sqrt(variance(X[:, 1]))\n",
    "    X1_std_dev = np.sqrt(np.sum(np.square(X1_norm-np.mean(X1_norm)))/X1_norm.shape[0])\n",
    "    return np.c_[ X[:, 0], X[:, 1], (X1_norm/X1_std_dev)*X2_norm, X[:, 2], (1-X[:, 0]/X[:, 3]), X[:, 4] ]\n",
    "\n",
    "def model_3(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Construct a C Model 3 OLS Linear Regression object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : Array-like, np.ndarray, shape (N, n)\n",
    "        The full-size feature matrix.\n",
    "    Y : Array-like, np.ndarray, Shape (N)\n",
    "        The response vector.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    model : OLSLinearRegression\n",
    "        C Model 3 object wrapper.\n",
    "    \"\"\"\n",
    "    X = model_3_expression(X_train)\n",
    "    model = OLSLinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing linear regression for each custom model, printing out the resulting MAE and select the best model for further operations.\n",
    "\n",
    "shuffle_indices = np.random.default_rng().permutation(X_train.shape[0])\n",
    "X_shuffle = X_train[shuffle_indices]\n",
    "Y_shuffle = y_train[shuffle_indices]\n",
    "c_model = [model_1_expression, model_2_expression, model_3_expression]\n",
    "c_model_k_fold = [k_fold(c_model[0](X_shuffle), Y_shuffle, 10), \n",
    "                  k_fold(c_model[1](X_shuffle), Y_shuffle, 10),\n",
    "                  k_fold(c_model[2](X_shuffle), Y_shuffle, 10)\n",
    "                  ]\n",
    "c_gen_model = [model_1, model_2, model_3]\n",
    "c_models_repr = [\n",
    "    ('$$\\hat{{y}}={:.3f}x_1{}{:.3f}$$'),\n",
    "    ('$$\\hat{{y}}=2\\\\times {:.3f}\\sqrt{{x_1x_2+\\dfrac{{3}}{{8}}}}' + \\\n",
    "         '{}{:.3f}x_2' + \\\n",
    "         '{}{:.3f}x_3' + \\\n",
    "         '{}{:.3f}\\dfrac{{x_4}}{{x_1}}' + \\\n",
    "         '{}{:.3f}x_5' + \\\n",
    "         '{}{:.3f}$$ '),\n",
    "    ('$$\\hat{{y}}={:.3f}x_1' + \\\n",
    "         '{}{:.3f}x_2' + \\\n",
    "         '{}{:.3f}\\dfrac{{{{Norm}}(x_1)}}{{\\sigma_{{{{Norm}}(x_1)}}}}{{Norm}}(x_2)' + \\\n",
    "         '{}{:.3f}x_3' + \\\n",
    "         '{}{:.3f}\\dfrac{{x_1}}{{x_4}}' + \\\n",
    "         '{}{:.3f}x_5' + \\\n",
    "         '{}{:.3f}$$ ')\n",
    "]\n",
    "\n",
    "# formating model 2 and model 3 representation\n",
    "format_1 = ['']*(c_model_k_fold[1][0].get_params().shape[0]*2-1)\n",
    "format_1[0::2] = list(c_model_k_fold[1][0].get_params())\n",
    "format_1[1:(c_model_k_fold[1][0].get_params().shape[0]*2-2):2] = \\\n",
    "    ['+' if c_model_k_fold[1][0].get_params()[i] >= 0 else '' for i in range(1, c_model_k_fold[1][0].get_params().shape[0])]\n",
    "\n",
    "format_2 = ['']*(c_model_k_fold[2][0].get_params().shape[0]*2-1)\n",
    "format_2[0::2] = list(c_model_k_fold[2][0].get_params())\n",
    "format_2[1:(c_model_k_fold[2][0].get_params().shape[0]*2-2):2] = \\\n",
    "    ['+' if c_model_k_fold[2][0].get_params()[i] >= 0 else '' for i in range(1, c_model_k_fold[2][0].get_params().shape[0])]\n",
    "\n",
    "# tabulating data\n",
    "cross_validation_output = {\n",
    "    'Index': range(1,4),\n",
    "    'Model': [\n",
    "        c_models_repr[0].format(c_model_k_fold[0][0].get_params()[0],\n",
    "                                '+' if c_model_k_fold[0][0].get_params()[1] >= 0 else '',\n",
    "                                 c_model_k_fold[1][0].get_params()[1]),\n",
    "        c_models_repr[1].format(*format_1) +\n",
    "            'Parameters regression and regularization: $$\\hat{{\\\\beta}}=({{\\mathbf{{X}}}}^{{\\\\top}}\\mathbf{{X}}+\\\n",
    "            \\lambda \\mathbf{{\\mathit{{I}}}}){{\\mathbf{{X}}}}^{{\\\\top}}\\mathbf{{y}}$$',\n",
    "        c_models_repr[2].format(*format_2) +\n",
    "         '$$\\\\text{{Where}}\\quad {{Norm}}(U)=\\\\dfrac{{U-\\overline{{U}}}}{{\\sigma_U}}, \\quad \\\n",
    "            \\sigma_U= \\\\sqrt{{\\\\dfrac{{\\sum_{{u\\in U}} (u-\\\\overline{{U}})^2}}{{\\|U\\|}}}}$$'\n",
    "    ],\n",
    "    'MAE': np.round(np.array([c_model_k_fold[0][1], c_model_k_fold[1][1], c_model_k_fold[2][1]]), 3)\n",
    "}\n",
    "cv_table = tabulate(cross_validation_output, headers='keys', tablefmt='github')\n",
    "display(Markdown('<center>\\n\\n' + cv_table + '\\n\\n</center>'))\n",
    "\n",
    "# Extracting the index of the best feature (since we have been following the structural run of the codebase, \n",
    "# one can assumes that the index of this list aligns with the index of the feature)\n",
    "best_index = np.argmin([c_model_k_fold[0][1], c_model_k_fold[1][1], c_model_k_fold[2][1]])\n",
    "format_best = ['']*(c_model_k_fold[best_index][0].get_params().shape[0]*2-1)\n",
    "format_best[0::2] = list(c_model_k_fold[best_index][0].get_params())\n",
    "format_best[1:(c_model_k_fold[best_index][0].get_params().shape[0]*2-2):2] = \\\n",
    "    ['+' if c_model_k_fold[2][0].get_params()[i] >= 0 else '' for i in range(1, c_model_k_fold[2][0].get_params().shape[0])]\n",
    "\n",
    "display(Markdown(f'<center>\\n\\nBest candidate model: {c_models_repr[best_index].format(*format_best)}\\n\\n</center>'))\n",
    "# remember to normalize test input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model\n",
    "\n",
    "X_best_train = X_train\n",
    "# Best fit polynomial of degree 1 OLS Linear Regression model\n",
    "# The keyword ridge_lambda presents here to accomodate model's ridge_lambda parameter,\n",
    "# since all model procedure already included the **kwargs parameter, they can safely ignore it if not needed\n",
    "my_best_model = c_gen_model[best_index](c_model[best_index](X_train), y_train, ridge_lambda=0.01)\n",
    "format_best = ['']*(my_best_model.get_params().shape[0]*2-1)\n",
    "format_best[0::2] = list(my_best_model.get_params())\n",
    "format_best[1:(my_best_model.get_params().shape[0]*2-2):2] = \\\n",
    "    ['+' if my_best_model.get_params()[i] >= 0 else '' for i in range(1, my_best_model.get_params().shape[0])]\n",
    "display(Markdown(f'<center>\\n\\nBest model: {c_models_repr[best_index].format(*format_best)}\\n\\n</center>'))\n",
    "\n",
    "# \"\"\" Plot the resulting model\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "handle_1 = ax.plot(X_test, y_test, 'ro', label='Ground truth data')\n",
    "handle_2 = ax.plot(X_test, my_best_model.predict(c_model[best_index](X_test)), 'bo', label='Model prediction')\n",
    "ax.set_xlabel(f'x')\n",
    "ax.set_ylabel(f'y')\n",
    "ax.legend(handles=[handle_1[0], handle_2[0]])\n",
    "ax.set_title('Linear regression by OLS model ' + str(best_index+1), loc=\"center\")\n",
    "plt.draw()\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Latex('$$\\\\text{{MAE:}} \\quad {mae:.3f}$$'.format(mae=ols_fitness_stats(my_best_model, c_model[best_index](X_test), y_test)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Student Performance} = 3.291x_1+1.018x_22−0.017\\dfrac{{Norm}(x_1)}{\\sigma_{{Norm}(x_1)}}{Norm}(x_2)+0.595x_3+2.676\\dfrac{x1}{x4}+0.194x_5−33.563$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15af99fd1a1a3f0a3416ea421564e792a8676a13670c2eed127d89ab0518a27b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
